name: ETL Pipeline CI/CD

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]
  schedule:
    # Run tests daily at 6 AM UTC
    - cron: '0 6 * * *'

env:
  PYTHON_VERSION: '3.11'
  AIRFLOW__CORE__UNIT_TEST_MODE: 'True'

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest-cov

      - name: Run linter
        run: |
          pip install ruff
          ruff check .

      - name: Run tests
        run: |
          pytest --cov=./ --cov-report=xml --cov-report=term tests/

      - name: Upload coverage report
        uses: codecov/codecov-action@v3
        if: always()
        with:
          files: ./coverage.xml
          flags: unittests
          name: codecov-umbrella

  run-etl:
    needs: test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run ETL Pipeline
        run: |
          python etl_pipeline.py

      - name: Verify outputs
        run: |
          if [ -f "output/clean_events.parquet" ] && [ -f "output/daily_summary.parquet" ]; then
            echo "ETL outputs created successfully"
            python -c "
            import pyarrow.parquet as pq
            clean = pq.read_table('output/clean_events.parquet')
            summary = pq.read_table('output/daily_summary.parquet')
            print(f'Clean events: {len(clean)} rows')
            print(f'Daily summary: {len(summary)} rows')
            "
          else
            echo "ERROR: ETL outputs not found"
            exit 1
          fi

      - name: Archive outputs
        uses: actions/upload-artifact@v4
        with:
          name: etl-outputs
          path: output/
          retention-days: 7

  notify:
    needs: [test, run-etl]
    if: always()
    runs-on: ubuntu-latest
    steps:
      - name: Send notification on failure
        if: failure()
        run: |
          echo "ETL Pipeline workflow failed!"
          # Add your notification logic here (Slack, email, etc.)
